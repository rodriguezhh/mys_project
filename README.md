# Mini Project: Modeling & Simulation
## Optimization: Comparison between batch, stochastic and mini-batch techniques
---
**Created by:** Harold H. Rodriguez R.\
**Contact:** rodriguezhh03@gmail.com

The comparison of different optimization techniques is fundamental to thoroughly understand the functioning of each one and determine which best suits the specific needs of each problem in training machine learning models. This document presents the comparison of these techniques in two phases: in the first phase, models are trained using the same parameters and hyperparameters for the batch, stochastic, and mini-batch techniques, employing gradient descent as the optimizer, with the objective of selecting the technique that performs best for the specific problem. In the second phase, the selected technique is applied with different optimizers to train the models with the same data.
